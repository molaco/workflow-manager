project:
  name: "[PROJECT_NAME]"
  description: "[Brief description of what this project/feature accomplishes]"
  total_tasks: [NUMBER]
  estimated_total_tests: [NUMBER]

testing_stack:
  - tool: "[Tool/Framework name]"
    purpose: "[What it's used for - e.g., property-based testing, model checking, etc.]"
    scope: "[Which tasks/components use this - e.g., Tasks 1, 3, 5 or 'All tasks']"

  # Repeat for each tool in the testing stack
  # Common tools: PropTest, Kani, Loom, Standard Unit Tests, Integration Tests, etc.

verification_summary:
  tiers:
    tier_1:
      priority: "[Critical - Must Verify]"
      targets:
        - task_id: [NUMBER]
          component: "[Component/function name]"
          concern: "[What needs to be verified - e.g., correctness, safety, etc.]"
          method: "[Verification method - e.g., model checking, property tests]"

        # Repeat for each tier 1 target

    tier_2:
      priority: "[Important - Should Verify]"
      targets:
        - task_id: [NUMBER]
          component: "[Component/function name]"
          concern: "[What needs to be verified]"
          method: "[Verification method]"

        # Repeat for each tier 2 target

    tier_3:
      priority: "[Nice to Have]"
      targets:
        - task_id: [NUMBER]
          component: "[Component/function name]"
          concern: "[What needs to be verified]"
          method: "[Verification method]"

        # Repeat for each tier 3 target

  recommended_tools:
    - name: "[Tool name]"
      use_case: "[Specific use case for this project]"

    # Repeat for each recommended tool

  runtime_verification:
    # List of invariants to check at runtime (debug assertions)
    - "[Invariant 1 - e.g., Data structure maintains sorted order]"
    - "[Invariant 2 - e.g., Values remain positive]"
    - "[Invariant 3 - e.g., State machine constraints]"
    # Continue for each runtime check

testing_methodologies:
  property_based_testing:
    description: |
      [Detailed explanation of property-based testing approach.
      Explain how it generates random inputs to verify properties hold universally.]

    when_to_use:
      - "[Use case 1 - e.g., Testing mathematical properties (commutativity, associativity)]"
      - "[Use case 2 - e.g., Testing serialization/deserialization roundtrips]"
      - "[Use case 3 - e.g., Testing invariants across wide input ranges]"

    tools:
      - "[Tool 1 - e.g., PropTest]"
      - "[Tool 2 - e.g., QuickCheck]"

    example_patterns:
      - pattern: "[Pattern name - e.g., Roundtrip/Bijection]"
        description: "[What it tests - e.g., serialize(deserialize(x)) == x]"
        template: |
          [Code template or pseudocode example]
          proptest! {
              #[test]
              fn prop_roundtrip(input in strategy()) {
                  assert_eq!(reverse(reverse(input)), input);
              }
          }

      - pattern: "[Pattern name - e.g., Invariant preservation]"
        description: "[What it tests - e.g., function maintains data structure invariants]"
        template: |
          [Code template]

      # Add more patterns as needed

    best_practices:
      - "[Practice 1 - e.g., Start with simple properties before complex ones]"
      - "[Practice 2 - e.g., Use shrinking to find minimal failing cases]"
      - "[Practice 3 - e.g., Run 1000+ test cases for confidence]"

  unit_testing:
    description: |
      [Explanation of unit testing approach.
      Focus on testing individual functions/components in isolation.]

    when_to_use:
      - "[Use case 1 - e.g., Testing specific edge cases]"
      - "[Use case 2 - e.g., Testing error handling paths]"
      - "[Use case 3 - e.g., Testing mock interactions]"

    tools:
      - "[Tool 1 - e.g., Built-in Rust test framework]"
      - "[Tool 2 - e.g., Mockall for mocking]"

    example_patterns:
      - pattern: "[Pattern name - e.g., Exhaustive enum testing]"
        description: "[What it tests - e.g., all enum variants behave correctly]"
        template: |
          [Code template]
          #[test]
          fn test_all_variants() {
              for variant in Enum::ALL {
                  assert!(variant.some_property());
              }
          }

      # Add more patterns

    best_practices:
      - "[Practice 1]"
      - "[Practice 2]"

  integration_testing:
    description: |
      [Explanation of integration testing approach.
      Testing how components work together, often with real external services.]

    when_to_use:
      - "[Use case 1 - e.g., Testing WebSocket connection flows]"
      - "[Use case 2 - e.g., Testing API integration]"
      - "[Use case 3 - e.g., Testing database interactions]"

    tools:
      - "[Tool 1]"

    example_patterns:
      - pattern: "[Pattern name - e.g., Real API connection test]"
        description: "[What it tests]"
        template: |
          [Code template]
          #[test]
          #[ignore]  // Requires real API access
          fn test_real_connection() {
              // Test implementation
          }

      # Add more patterns

    best_practices:
      - "[Practice 1 - e.g., Use #[ignore] for tests requiring external services]"
      - "[Practice 2 - e.g., Run integration tests sequentially with --test-threads=1]"

  model_checking:
    description: |
      [Explanation of model checking with formal verification tools.
      Mathematically proves properties hold for ALL possible inputs.]

    when_to_use:
      - "[Use case 1 - e.g., Critical safety properties (no overflow, no panic)]"
      - "[Use case 2 - e.g., Complex state machine verification]"
      - "[Use case 3 - e.g., Sequence/ordering guarantees]"

    tools:
      - "[Tool 1 - e.g., Kani Rust Verifier]"
      - "[Tool 2 - e.g., CBMC]"

    example_patterns:
      - pattern: "[Pattern name - e.g., Overflow-free arithmetic]"
        description: "[What it proves - e.g., operations never overflow]"
        template: |
          [Code template]
          #[kani::proof]
          fn verify_no_overflow() {
              let a: u32 = kani::any();
              let b: u32 = kani::any();
              kani::assume(a <= MAX && b <= MAX);
              let result = checked_add(a, b);
              assert!(result.is_some());
          }

      # Add more patterns

    best_practices:
      - "[Practice 1 - e.g., Use bounded inputs to make verification tractable]"
      - "[Practice 2 - e.g., Focus on critical safety properties]"

  concurrency_testing:
    description: |
      [Explanation of concurrency testing approach.
      Testing thread safety and detecting race conditions.]

    when_to_use:
      - "[Use case 1 - e.g., Testing locks and synchronization]"
      - "[Use case 2 - e.g., Testing lock-free data structures]"
      - "[Use case 3 - e.g., Testing rate limiters and shared state]"

    tools:
      - "[Tool 1 - e.g., Loom]"
      - "[Tool 2 - e.g., ThreadSanitizer]"

    example_patterns:
      - pattern: "[Pattern name - e.g., Race condition detection]"
        description: "[What it tests]"
        template: |
          [Code template]
          #[test]
          #[cfg(loom)]
          fn test_concurrent_access() {
              loom::model(|| {
                  // Concurrent operations
              });
          }

      # Add more patterns

    best_practices:
      - "[Practice 1 - e.g., Run with RUSTFLAGS=\"--cfg loom\"]"
      - "[Practice 2 - e.g., Test all interleavings systematically]"

testing_guidelines:
  strategies:
    - name: "[Strategy name - e.g., Exhaustive enum testing]"
      applicable_to: "[What types of code - e.g., Finite enums with all variants known]"
      approach: |
        [How to implement this strategy.
        Step-by-step guidance on applying this testing approach.]
      tools:
        - "[Tool 1]"
        - "[Tool 2]"
      example: |
        [Code snippet or reference to example implementation]

    - name: "[Strategy name - e.g., Parser testing]"
      applicable_to: "[e.g., Functions that parse/deserialize data]"
      approach: |
        [Implementation approach]
      tools:
        - "[Tool 1]"
      example: |
        [Code example]

    # Add more strategies as needed

  best_practices:
    - "[Practice 1 - e.g., Write tests before or alongside implementation (TDD)]"
    - "[Practice 2 - e.g., Test both happy path and error cases]"
    - "[Practice 3 - e.g., Use descriptive test names that explain what is being tested]"
    - "[Practice 4 - e.g., Keep tests independent and isolated]"
    - "[Practice 5 - e.g., Prefer property tests over example-based tests when applicable]"
    # Add more best practices

  coverage_requirements:
    minimum: [PERCENTAGE]  # e.g., 80
    target: [PERCENTAGE]   # e.g., 90
    critical_paths: [PERCENTAGE]  # e.g., 100
    notes: |
      [Additional context about coverage requirements.
      Explain what needs 100% coverage vs what can have lower coverage.]

dependency_graph:
  description: |
    [Brief explanation of the dependency structure and what it represents.
    Describe any parallel execution opportunities or critical paths.]

  critical_path:
    # Longest chain of dependent tasks
    - [TASK_NUMBER]
    - [TASK_NUMBER]
    - [TASK_NUMBER]
    # Continue listing tasks in dependency order

  parallelizable_after_task_[NUMBER]:
    # Tasks that can be done in parallel after a specific task completes
    - [TASK_NUMBER]
    - [TASK_NUMBER]
    # Repeat for each parallelizable group

  foundation_task: [TASK_NUMBER]  # The base task everything depends on
  integration_task: [TASK_NUMBER]  # Task that brings everything together
  validation_task: [TASK_NUMBER]   # Final testing/validation task

  graph: |
    [ASCII or text representation of the dependency graph]
    Example:
    Task 1 (Foundation)
      ↓
      ├─→ Task 2 ────→ Task 5
      │                  ↓
      ├─→ Task 3 ────→ Task 6
      │                  ↓
      └─→ Task 4 ────→ Task 7 (Integration)
                         ↓
                      Task 8 (Validation)

test_execution:
  commands:
    - name: "[Description of what this command does]"
      command: "[Actual command to run]"
      scope: "[Optional: specific task or component]"
      note: "[Optional: additional context or requirements]"

    # Common commands to include:
    # - Run all unit tests
    # - Run property tests
    # - Run concurrency tests
    # - Run model checking
    # - Run integration tests
    # - Run with coverage
    # - Run linter/clippy
    # Repeat for each command

  pre_merge_checklist:
    - item: "[Checklist item 1]"
      critical: [true|false]
      scope: "[Optional: specific task this applies to]"

    - item: "[Checklist item 2]"
      critical: [true|false]
      scope: "[Optional: specific task this applies to]"

    # Common checklist items:
    # - All unit tests pass
    # - All property tests pass
    # - Integration tests pass
    # - No compiler warnings
    # - Code coverage meets threshold
    # - Documentation updated
    # - Manual testing completed
    # Repeat for each checklist item

task_summaries:
  - id: [NUMBER]
    name: "[TASK_NAME]"
    summary: "[1-2 sentence description of what this task accomplishes]"
    test_count: [NUMBER]
    critical: [true|false]  # Optional: mark critical tasks

  # Repeat for each task in the project
  # This provides a quick reference without reading full task files

notes:
  # Optional: Additional project-level notes
  architectural_decisions:
    - "[Key decision 1 and rationale]"
    - "[Key decision 2 and rationale]"

  known_limitations:
    - "[Limitation 1]"
    - "[Limitation 2]"

  future_work:
    - "[Potential enhancement 1]"
    - "[Potential enhancement 2]"
