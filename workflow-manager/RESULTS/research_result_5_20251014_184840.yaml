workflow_execution_analysis:
  overview: |
    The workflow manager uses a process-based execution model where workflows are compiled 
    as separate binaries (in src/bin/) and spawned as child processes by the TUI manager. 
    Communication happens via stderr parsing for structured logs (__WF_EVENT__ protocol) 
    and stdout for text output. The system also includes in-process agent execution via 
    the claude-agent-sdk for AI-powered workflows.

execution_patterns:
  process_based_workflows:
    description: "Primary execution model - workflows as separate OS processes"
    
    examples:
      simple_echo:
        path: "src/bin/simple_echo.rs"
        pattern: "Standalone binary using WorkflowDefinition macro"
        execution: "Tokio async main, prints to stdout, no agents"
        features:
          - "CLI argument parsing via clap"
          - "WorkflowDefinition derive macro for metadata"
          - "Direct stdout/stderr output"
          - "Simple async operations (file I/O, delays)"
      
      hooks_demo:
        path: "src/bin/hooks_demo.rs"
        pattern: "In-process agent execution with hooks"
        execution: "Creates ClaudeSDKClient with hook callbacks"
        features:
          - "Uses claude-agent-sdk::ClaudeSDKClient"
          - "Registers PreToolUse/PostToolUse hooks"
          - "Command validation (blocks dangerous rm)"
          - "Audit logging for tool usage"
          - "Multiple hook matchers per event"
        
      demo_multiphase:
        path: "src/bin/demo_multiphase.rs"
        pattern: "Hierarchical phase/task structure with logging"
        execution: "Emits WorkflowLog events to stderr for TUI parsing"
        features:
          - "Phase-based organization (0-2)"
          - "Task tracking with progress updates"
          - "Simulated agent execution (validator, formatter, reviewer)"
          - "State file logging for intermediate outputs"
          - "Structured event emission via log_* macros"
      
      research_agent:
        path: "src/bin/research_agent.rs"
        pattern: "Complex multi-phase research workflow with concurrent agents"
        execution: "Map-reduce pattern with parallel agent execution"
        features:
          - "5 phases: analyze→prompts→research→validate→synthesize"
          - "Uses claude-agent-sdk::query for agent execution"
          - "Concurrent execution with Semaphore rate limiting"
          - "State resumption from YAML files"
          - "Phase selector for partial execution"
          - "FuturesUnordered for parallel task execution"
  
  in_process_agent_execution:
    description: "AI agents executed in-process via claude-agent-sdk"
    
    sdk_patterns:
      query_function:
        usage: "claude_agent_sdk::query(prompt, options) -> Stream<Message>"
        location: "research_agent.rs:379, 456, 529"
        pattern: "Stateless query - returns message stream"
        lifecycle: "Create options → query → consume stream → complete"
      
      client_with_state:
        usage: "ClaudeSDKClient::new(options, context) -> Client"
        location: "hooks_demo.rs:191"
        pattern: "Stateful client for multiple interactions"
        methods:
          - "send_message(text) -> Result"
          - "next_message() -> Option<Message>"
        lifecycle: "Create client → send messages → poll next_message → consume Result"
    
    configuration:
      ClaudeAgentOptions:
        builder_pattern: true
        key_options:
          - "system_prompt: String | SystemPrompt::Preset"
          - "max_turns: u32"
          - "allowed_tools: Vec<String>"
          - "permission_mode: BypassPermissions | AskForPermission"
          - "hooks: HashMap<HookEvent, Vec<HookMatcher>>"
        
      hooks:
        events:
          - "PreToolUse: Before tool execution"
          - "PostToolUse: After tool completes"
        matcher:
          tool_specific: "matcher: Some('Bash')"
          all_tools: "matcher: None"
        callback_signature: "Fn(input, tool_name, context) -> Pin<Box<Future<Result<HookOutput>>>>"

workflow_state_management:
  tui_level_state:
    WorkflowTab:
      location: "src/main.rs:89"
      fields:
        identity:
          - "id: String (unique timestamp-based)"
          - "workflow_idx: usize (index in catalog)"
          - "workflow_name: String"
          - "instance_number: usize (per-workflow counter)"
        execution:
          - "child_process: Option<std::process::Child>"
          - "status: WorkflowStatus"
          - "exit_code: Option<i32>"
          - "start_time: Option<DateTime>"
        output:
          - "workflow_output: Arc<Mutex<Vec<String>>>"
          - "workflow_phases: Arc<Mutex<Vec<Phase>>>"
        ui_state:
          - "scroll_offset: usize"
          - "expanded_phases/tasks/agents: HashSet<usize>"
          - "selected_phase/task/agent"
        persistence:
          - "field_values: HashMap<String, String>"
          - "saved_logs: Option<String>"
  
  workflow_level_state:
    structured_logging:
      protocol: "__WF_EVENT__:<JSON>"
      encoding: "Line-delimited JSON on stderr"
      parser: "line.strip_prefix('__WF_EVENT__:') → serde_json::from_str"
      location: "src/main.rs:536, 878, 1035"
      
    event_types:
      phase_events:
        - "PhaseStarted { phase, name, total_phases }"
        - "PhaseCompleted { phase, name }"
        - "PhaseFailed { phase, name, error }"
      
      task_events:
        - "TaskStarted { phase, task_id, description, total_tasks }"
        - "TaskProgress { task_id, message }"
        - "TaskCompleted { task_id, result }"
        - "TaskFailed { task_id, error }"
      
      agent_events:
        - "AgentStarted { task_id, agent_name, description }"
        - "AgentMessage { task_id, agent_name, message }"
        - "AgentCompleted { task_id, agent_name, result }"
        - "AgentFailed { task_id, agent_name, error }"
      
      state_events:
        - "StateFileCreated { phase, file_path, description }"
    
    event_handling:
      location: "src/main.rs:handle_workflow_event"
      mechanism: "Updates Arc<Mutex<Vec<Phase>>> hierarchically"
      creates_on_demand: "Phases, tasks, and agents created when first event received"
    
    sdk_macros:
      provided_by: "workflow-manager-sdk/src/lib.rs:206-369"
      macros:
        - "log_phase_start!(phase, name, total)"
        - "log_phase_complete!(phase, name)"
        - "log_task_start!(phase, task_id, desc)"
        - "log_task_progress!(task_id, message)"
        - "log_task_complete!(task_id, result)"
        - "log_agent_start!(task_id, agent, desc)"
        - "log_agent_message!(task_id, agent, msg)"
        - "log_agent_complete!(task_id, agent, result)"
        - "log_state_file!(phase, path, desc)"
      
      implementation: |
        WorkflowLog::emit() {
          eprintln!("__WF_EVENT__:{}", serde_json::to_string(self));
          std::io::stderr().flush();
        }

process_lifecycle_control:
  spawning:
    method: "std::process::Command"
    location: "src/main.rs:500, 844, 998"
    configuration:
      stdin: "Stdio::null()"
      stdout: "Stdio::piped()"
      stderr: "Stdio::piped()"
    
    workflow:
      - "Build CLI args from field_values HashMap"
      - "Spawn Command with piped stdio"
      - "Take stdout/stderr handles from child"
      - "Spawn threads to read each stream"
      - "Store child_process handle in WorkflowTab"
    
    thread_architecture:
      stdout_reader:
        code: "src/main.rs:514"
        mechanism: "BufReader::lines() → push to workflow_output"
        blocking: true
        lifetime: "Until stdout closes (process exits)"
      
      stderr_reader:
        code: "src/main.rs:531"
        mechanism: "BufReader::lines() → parse __WF_EVENT__ → update phases"
        blocking: true
        lifetime: "Until stderr closes (process exits)"
        parsing: |
          if line.starts_with("__WF_EVENT__:") {
            deserialize WorkflowLog → handle_workflow_event(event, phases)
          } else {
            treat as error → push to workflow_output
          }
  
  monitoring:
    status_tracking:
      mechanism: "WorkflowStatus enum in WorkflowTab"
      states:
        - "NotStarted"
        - "Running"
        - "Completed"
        - "Failed"
      updates: "Manual in TUI, based on spawn success/failure"
    
    exit_detection:
      method: "Child::try_wait() polling"
      location: "src/main.rs:644"
      frequency: "Every TUI render loop iteration"
      on_exit: "Sets status to Completed/Failed based on exit code"
  
  cancellation:
    user_initiated:
      trigger: "Close tab confirmation or kill command"
      mechanism: "child.kill() → sends SIGKILL"
      location: "src/main.rs:407, 431, 450"
      limitations:
        - "No graceful shutdown (no SIGTERM first)"
        - "No grace period or timeout"
        - "Immediate SIGKILL - workflows cannot cleanup"
    
    cleanup:
      tab_closure:
        code: "src/main.rs:406"
        workflow: |
          if let Some(mut child) = tab.child_process.take() {
            let _ = child.kill();
          }
      
      rerun:
        code: "src/main.rs:449"
        workflow: "Kill old process → clear output → spawn new"
  
  no_automatic_restart: "Crashed workflows stay in Failed state"
  no_timeout_enforcement: "Long-running workflows never auto-killed"

api_design_recommendations:
  dual_mode_support:
    rationale: |
      Current system has TWO distinct execution models that serve different purposes:
      
      1. Process-based (simple_echo, demo_multiphase):
         - Isolation, security, resource control
         - Clear lifecycle (spawn → exit)
         - Easy cancellation (SIGKILL)
         - OS-managed resources
      
      2. Library-based (hooks_demo internals, research_agent agents):
         - Code reuse (use workflow logic in other apps)
         - Testing without process overhead
         - Shared memory / zero-copy data
         - Cooperative cancellation needed
    
    recommendation: "Support BOTH, make mode explicit in API"
    
  proposed_trait:
    code: |
      pub trait WorkflowExecutor {
          // Core execution
          async fn execute(&mut self, params: WorkflowParams) -> Result<WorkflowHandle>;
          
          // Lifecycle control
          async fn cancel(&mut self, handle: &WorkflowHandle) -> Result<()>;
          async fn wait(&mut self, handle: &WorkflowHandle) -> Result<WorkflowResult>;
          
          // Monitoring
          fn status(&self, handle: &WorkflowHandle) -> WorkflowStatus;
          fn logs(&self, handle: &WorkflowHandle) -> impl Stream<Item = WorkflowLog>;
          
          // Capabilities
          fn supports_sandboxing(&self) -> bool;
          fn supports_resource_limits(&self) -> bool;
      }
    
    implementations:
      ProcessExecutor:
        spawning: "std::process::Command"
        cancellation: "SIGTERM → grace period → SIGKILL"
        sandboxing: "setrlimit, cgroups (Linux), sandboxd (macOS)"
        resource_limits: "rlimit, cgroups v2"
        logs: "Parse stderr for __WF_EVENT__"
      
      LibraryExecutor:
        spawning: "Direct function call / tokio::spawn"
        cancellation: "CancellationToken cooperative"
        sandboxing: "Not possible (same process)"
        resource_limits: "Manual tracking only"
        logs: "Direct WorkflowLog stream"
  
  params_structure:
    code: |
      pub struct WorkflowParams {
          pub field_values: HashMap<String, String>,
          pub resource_limits: Option<ResourceLimits>,
          pub timeout: Option<Duration>,
          pub cancellation_token: Option<CancellationToken>,
      }
      
      pub struct ResourceLimits {
          pub max_cpu_time: Option<Duration>,
          pub max_memory_bytes: Option<u64>,
          pub max_file_descriptors: Option<u32>,
          pub max_processes: Option<u32>,
      }
  
  handle_structure:
    code: |
      pub struct WorkflowHandle {
          pub id: WorkflowId,
          pub mode: ExecutionMode,
          inner: Box<dyn Any>, // ProcessHandle or TaskHandle
      }
      
      pub enum ExecutionMode {
          Process { pid: u32 },
          InProcess { task_id: u64 },
      }

sandboxing_strategies:
  process_isolation:
    benefits:
      - "OS-level privilege separation"
      - "Resource accounting via /proc"
      - "Clean termination on kill"
      - "Cannot affect parent process"
    
    mechanisms:
      linux:
        rlimit:
          method: "setrlimit before exec"
          supports: ["CPU time", "memory (RLIMIT_AS)", "file descriptors", "processes"]
          code: "Command::new().pre_exec(|| setrlimit(RLIMIT_CPU, ...))"
        
        cgroups_v2:
          method: "Write to /sys/fs/cgroup hierarchy"
          supports: ["CPU shares", "memory limit", "I/O bandwidth", "process count"]
          setup: "Create cgroup → write limits → move process to cgroup"
          cleanup: "Remove cgroup on exit"
        
        seccomp:
          method: "Syscall filtering"
          use_case: "Block dangerous syscalls (execve, fork for no-spawn workflows)"
        
        namespaces:
          method: "unshare() for isolation"
          types: ["PID (process tree)", "mount (filesystem)", "network", "IPC"]
      
      macos:
        sandbox_init:
          method: "sandbox-exec or sandbox_init(3)"
          profile: "SBPL (Sandbox Profile Language)"
          limitations: "Complex, poorly documented"
        
        rlimit:
          method: "Same as Linux"
          note: "Limited compared to Linux"
      
      windows:
        job_objects:
          method: "CreateJobObject / AssignProcessToJobObject"
          supports: ["CPU time", "memory", "process count", "I/O rate"]
    
    current_limitation: "NO sandboxing - workflows have full parent privileges"
  
  library_mode_challenges:
    no_isolation: "Shares address space with parent"
    resource_tracking: "Must manually track allocations"
    cancellation: "Requires cooperative checks (tokio::select! with token)"
    dangerous_operations:
      - "Can call exit() and kill parent"
      - "Can corrupt parent memory"
      - "Shared file descriptors"
    
    mitigation_required:
      - "Code review / trust workflows"
      - "Wasm sandbox for untrusted code"
      - "CancellationToken in all async branches"

cancellation_design:
  process_based:
    graceful_shutdown:
      workflow:
        - "Send SIGTERM to process"
        - "Wait grace period (5-30 seconds)"
        - "If still running, send SIGKILL"
        - "Wait for exit"
      
      workflow_side_handling:
        code: |
          use tokio::signal::unix::{signal, SignalKind};
          
          let mut term_signal = signal(SignalKind::terminate())?;
          
          tokio::select! {
              _ = term_signal.recv() => {
                  log_phase_failed!(current_phase, "Cancelled", "SIGTERM");
                  cleanup_resources().await?;
                  std::process::exit(130); // 128 + SIGTERM(2)
              }
              result = run_workflow() => result
          }
      
      benefits:
        - "Workflows can save intermediate state"
        - "Close file handles properly"
        - "Send cleanup events to TUI"
    
    current_implementation: "Direct SIGKILL (immediate termination)"
  
  library_based:
    token_pattern:
      code: |
        pub struct CancellationToken {
            inner: Arc<tokio::sync::Notify>,
        }
        
        impl CancellationToken {
            pub fn cancel(&self) {
                self.inner.notify_waiters();
            }
            
            pub async fn cancelled(&self) {
                self.inner.notified().await;
            }
        }
        
        // Usage in workflow
        async fn phase_1(cancel: CancellationToken) -> Result<()> {
            for item in items {
                tokio::select! {
                    _ = cancel.cancelled() => {
                        return Err(anyhow!("Cancelled during phase 1"));
                    }
                    result = process_item(item) => {
                        result?;
                    }
                }
            }
            Ok(())
        }
      
      workflow_integration: "Pass token to all async functions"
    
    challenges:
      - "Workflows must check token regularly"
      - "Blocking operations (non-async) cannot be cancelled"
      - "Third-party libraries may not support cancellation"

resource_limits_implementation:
  priority_1_cpu_time:
    rationale: "Prevent infinite loops from consuming 100% CPU forever"
    
    linux:
      method: "RLIMIT_CPU via setrlimit"
      code: |
        use nix::sys::resource::{setrlimit, Resource};
        
        Command::new(binary)
            .pre_exec(|| {
                setrlimit(Resource::RLIMIT_CPU, 300, 300)?; // 5 min
                Ok(())
            })
            .spawn()
      enforcement: "SIGXCPU after limit, SIGKILL after hard limit"
    
    alternative_tokio_timeout:
      code: |
        match tokio::time::timeout(Duration::from_secs(300), child.wait()).await {
            Ok(status) => handle_exit(status),
            Err(_) => {
                child.kill().await?;
                Err(anyhow!("Workflow timeout"))
            }
        }
  
  priority_2_memory:
    rationale: "Prevent OOM crashes from affecting TUI"
    
    linux:
      cgroups_v2:
        code: |
          // After spawning child
          let pid = child.id().unwrap();
          let cgroup_path = format!("/sys/fs/cgroup/workflow_{}", pid);
          
          std::fs::create_dir(&cgroup_path)?;
          std::fs::write(
              format!("{}/memory.max", cgroup_path),
              "1073741824" // 1 GB
          )?;
          std::fs::write(
              format!("{}/cgroup.procs", cgroup_path),
              pid.to_string()
          )?;
        
        cleanup: "Remove cgroup directory after wait()"
      
      rlimit_as:
        code: "setrlimit(RLIMIT_AS, 1GB, 1GB)"
        note: "Includes all memory (code, heap, stack, mmap)"
  
  priority_3_process_count:
    rationale: "Prevent fork bomb from workflows that spawn subprocesses"
    code: "setrlimit(RLIMIT_NPROC, 10, 10)"

  api_design:
    configuration:
      code: |
        pub struct ResourceLimits {
            pub cpu_time_seconds: Option<u64>,
            pub memory_bytes: Option<u64>,
            pub process_count: Option<u32>,
            pub file_descriptors: Option<u32>,
        }
        
        impl ProcessExecutor {
            pub fn with_limits(self, limits: ResourceLimits) -> Self {
                self.limits = Some(limits);
                self
            }
        }
      
      usage:
        code: |
          let executor = ProcessExecutor::new()
              .with_limits(ResourceLimits {
                  cpu_time_seconds: Some(300),
                  memory_bytes: Some(1_073_741_824),
                  process_count: Some(5),
                  file_descriptors: None,
              });
          
          executor.execute(workflow, params).await?;

current_system_gaps:
  no_sandboxing:
    issue: "Workflows run with full privileges of TUI process"
    risk: "Malicious workflow can delete files, exfiltrate data, spawn miners"
    mitigation: "User controls workflow binaries, not arbitrary code execution"
  
  no_resource_limits:
    issue: "Workflows can consume 100% CPU, all memory, spawn unlimited processes"
    risk: "Runaway workflow crashes system or makes TUI unresponsive"
    impact: "Research agent with batch_size=100 could spawn 100 Claude agents"
  
  no_graceful_cancellation:
    issue: "SIGKILL only - workflows cannot cleanup"
    impact:
      - "Intermediate state files not saved"
      - "Locks not released"
      - "Cannot send final events to TUI"
  
  no_timeout_enforcement:
    issue: "Long-running workflows never auto-killed"
    scenario: "Research agent stuck waiting for Claude API → runs forever"
  
  process_exit_only_status:
    issue: "Exit code is only signal of success/failure"
    limitation: "Cannot distinguish crash vs normal failure vs timeout"

recommendations:
  phase_1_critical:
    graceful_cancellation:
      effort: "Medium"
      implementation:
        - "TUI: Send SIGTERM before SIGKILL"
        - "Add 5-second grace period"
        - "Workflows: Add signal handlers (tokio::signal)"
      benefits: "Workflows can save state, proper cleanup"
    
    basic_resource_limits:
      effort: "Low"
      implementation:
        - "Add RLIMIT_CPU via pre_exec"
        - "Add tokio::timeout as fallback"
        - "Make limits configurable per workflow"
      benefits: "Prevent runaway CPU usage"
  
  phase_2_important:
    memory_limits:
      effort: "Medium (Linux), High (macOS/Windows)"
      implementation:
        - "Linux: cgroups v2 integration"
        - "macOS: rlimit only (less reliable)"
        - "Windows: Job objects"
      benefits: "Prevent OOM crashes"
    
    process_count_limits:
      effort: "Low"
      implementation: "RLIMIT_NPROC via pre_exec"
      benefits: "Prevent fork bombs"
  
  phase_3_advanced:
    sandboxing:
      effort: "High"
      implementation:
        - "Linux: seccomp + namespaces"
        - "macOS: sandbox-exec"
        - "Windows: AppContainer"
        - "Cross-platform: Wasm runtime for untrusted workflows"
      benefits: "Security for untrusted workflows"
    
    library_mode_api:
      effort: "Medium"
      implementation:
        - "Extract trait WorkflowExecutor"
        - "Implement LibraryExecutor"
        - "Pass CancellationToken to workflows"
        - "Update macros to work in both modes"
      benefits: "Code reuse, testing, embedding"
  
  phase_4_optional:
    distributed_execution:
      effort: "Very High"
      implementation: "RemoteExecutor using SSH or containers"
      benefits: "Scale workflows across machines"

concrete_examples:
  current_research_agent_risks:
    scenario: "User runs with --batch-size 20"
    consequences:
      - "Spawns 20 concurrent Claude agent queries"
      - "Each agent can use tools (Bash, Grep, Read)"
      - "If agents each do 10 file operations = 200 concurrent I/O"
      - "No CPU limit → 20 agents at 100% = 2000% CPU (on 20-core machine)"
      - "No memory limit → each agent 500MB = 10GB total"
      - "User cannot cancel gracefully → must SIGKILL losing all partial results"
    
    recommended_limits:
      cpu_time: "30 minutes total (covers map + reduce phases)"
      memory: "4 GB total (cgroup for entire process tree)"
      process_count: "batch_size + 5 (main + workers + a few tools)"
      graceful_shutdown: "SIGTERM → save research_results_partial.yaml → exit"

implementation_priorities:
  must_have_for_production:
    - "Graceful cancellation (SIGTERM + grace period)"
    - "CPU time limits (rlimit or tokio::timeout)"
    - "Memory limits (cgroups on Linux)"
  
  should_have_for_robustness:
    - "Process count limits"
    - "Configurable limits per workflow"
    - "Timeout display in TUI"
  
  nice_to_have_for_flexibility:
    - "Library execution mode"
    - "Full sandboxing"
    - "Distributed execution"

compatibility_notes:
  library_vs_process_tradeoffs:
    process_mode:
      pros:
        - "Strong isolation"
        - "OS-managed resources"
        - "Easy cancellation"
        - "Crash doesn't affect TUI"
      cons:
        - "Spawn overhead (10-50ms)"
        - "Cannot share memory"
        - "Separate address space"
    
    library_mode:
      pros:
        - "No spawn overhead"
        - "Can share data structures"
        - "Easy debugging (same process)"
        - "Synchronous return values"
      cons:
        - "No isolation (crash kills TUI)"
        - "No automatic resource limits"
        - "Harder to cancel"
        - "Shared file descriptors"
  
  recommendation_by_use_case:
    user_facing_workflows:
      mode: "Process-based"
      reason: "Isolation, security, user may not trust workflow code"
    
    internal_helper_workflows:
      mode: "Library-based"
      reason: "Performance, code reuse, workflow code is trusted"
    
    testing:
      mode: "Library-based"
      reason: "Fast iteration, no process overhead, synchronous"